{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import scipy.spatial.distance as sdist\n",
    "import umap\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree, fcluster\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import math\n",
    "import os\n",
    "import random\n",
    "#import requests\n",
    "#from BeautifulSoup import BeautifulSoup\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib as mpl\n",
    "from pylab import *\n",
    "#from ete3 import NCBITaxa\n",
    "#ncbi = NCBITaxa()\n",
    "import json\n",
    "from ast import literal_eval\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import scipy.signal\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "strain_color_list = [\"#33bbff\",\"#0050e6\",\"#009999\", \"#777777\"]\n",
    "umap_plot_shape = ['D','o','s','v']\n",
    "default_color_list = [u'#1f77b4', u'#ff7f0e', u'#2ca02c', u'#d62728', u'#9467bd', u'#8c564b', u'#e377c2', u'#7f7f7f', u'#bcbd22', u'#17becf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function \n",
    "# input is query_cluster, qseqid, qstart and qend\n",
    "# output is to print out corresponding read\n",
    "def get_sequence(assembly_dirt, assembly_file_name, qseqid, qstart, qend):\n",
    "    with open(assembly_dirt + assembly_file_name) as finput:\n",
    "        while True:\n",
    "            line_read = finput.readline()  \n",
    "            if len (line_read) == 0:\n",
    "                break\n",
    "            if \">\" in line_read:\n",
    "                if qseqid in line_read:\n",
    "                    line_read = ''\n",
    "                    while True:\n",
    "                        line_read_temp = finput.readline().strip()\n",
    "                        line_read = line_read+line_read_temp\n",
    "                        if len (line_read) == 0 or \">\" in line_read:\n",
    "                            break\n",
    "                    string_of_mobile_gene = line_read.strip()[qstart-1:qend]\n",
    "                    #print len(string_of_mobile_gene),string_of_mobile_gene\n",
    "                    return string_of_mobile_gene\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to take intervals and return merged intervals\n",
    "def merge_intervals(intervals):\n",
    "    sorted_by_lower_bound = sorted(intervals, key=lambda tup: tup[0])\n",
    "    merged = []\n",
    "\n",
    "    for higher in sorted_by_lower_bound:\n",
    "        if not merged:\n",
    "            merged.append(higher)\n",
    "        else:\n",
    "            lower = merged[-1]\n",
    "            # test for intersection between lower and higher:\n",
    "            # we know via sorting that lower[0] <= higher[0]\n",
    "            if higher[0] <= lower[1]:\n",
    "                upper_bound = max(lower[1], higher[1])\n",
    "                merged[-1] = (lower[0], upper_bound)  # replace by merged interval\n",
    "            else:\n",
    "                merged.append(higher)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the function to process data from variant calling from bcftools\n",
    "# inputs are name of vcf file, dirt and the two criteria to remove SNPs\n",
    "# 1st is the least number of cells that cover this SNP, \n",
    "# it is the cutoff_cell_count, default is 0.05*cell_number\n",
    "# 2nd is the cutoff to remove SNPs that either reference or alter is covered less than this nubmer of cells\n",
    "    # it is the larger value of (cutoff_type_count, cell_number*cutoff_cell_ratio)\n",
    "# Another input is the cutoff_SNP_ratio (ratio of SNPs to cover), \n",
    "# if a cell covers less than this ratio of SNPs, drop this cell\n",
    "# output files are: \n",
    "    # a dataframe of all SNP information\n",
    "    # a dataframe with less covered snps removed, and\n",
    "    # a dataframe with less covered SNPs removed and cells with less coverage removed\n",
    "# a plot of the SNP ratio\n",
    "def strain_split(file_name = '',work_dirt_temp = '', cutoff_cell_ratio=0.05, \\\n",
    "                 cutoff_type_count=2, cutoff_type_ratio=0.01,cutoff_SNP_ratio = 0.01):\n",
    "    # this is to parse vcf SNP file into lists\n",
    "    barcode_list = []\n",
    "    contig_list = []\n",
    "    location_on_contig_list = []\n",
    "    reference_list = []\n",
    "    alter_list = []\n",
    "    sequence_list = [] # the sequence at this location for this barcode\n",
    "    # it is a list of lists, each list element is a vector of all reads from this SNP, length is cell number\n",
    "    # if not mapped, give it 0\n",
    "    # if only 1 read covers this location, give it 0\n",
    "    # if less than 99% have the same snp call, give it 0\n",
    "    # same as reference is 1\n",
    "    # not the same as reference is -1\n",
    "    total_line = 0\n",
    "    with open(work_dirt_temp + file_name) as finput:\n",
    "        while True:\n",
    "            line_read = finput.readline()\n",
    "            if len (line_read) == 0:\n",
    "                break\n",
    "            # First parse and construct barcode list, and assign cutoff values\n",
    "            total_line += 1\n",
    "            if total_line == 4:\n",
    "                parsed_bam = line_read.split()\n",
    "                for i in range(len(parsed_bam)):\n",
    "                    barcode_with_bam = parsed_bam[i]\n",
    "                    if '.bam' == barcode_with_bam[-4:]:\n",
    "                        barcode_list += [barcode_with_bam[:-4]]\n",
    "                cutoff_cell_count = int(cutoff_cell_ratio*len(barcode_list))\n",
    "                cutoff_type_count = max(cutoff_type_count, \\\n",
    "                                       int(cutoff_type_ratio*len(barcode_list)))\n",
    "                \n",
    "            #print cutoff_cell_count\n",
    "            if line_read[0] != '#': # all information part starts with '#', so this is a line with mapping info\n",
    "                line_split_temp = line_read.split()\n",
    "                #print line_split_temp\n",
    "                contig_list += [int(line_split_temp[0].split('_')[1])]\n",
    "                location_on_contig_list += [int(line_split_temp[1])]\n",
    "                reference_list += [line_split_temp[3]]\n",
    "                alter_list += [line_split_temp[4]]\n",
    "                mapping_count_list_temp = [] # this is a temp count list for this SNP\n",
    "                # this is to assign value of each cell for each SNP\n",
    "                # if a cell has only 1 read for a SNP or less than 99% sure, assign 0\n",
    "                for i in range(9, len(line_split_temp)):\n",
    "                    mapping_result_temp = line_split_temp[i].split(\":\")[-1]\n",
    "                    reference_count_temp = int(mapping_result_temp.split(',')[0])\n",
    "                    alter_count_temp = int(mapping_result_temp.split(',')[1])\n",
    "                    total_count_temp = reference_count_temp + alter_count_temp\n",
    "                    if total_count_temp < 2:\n",
    "                        mapping_count_list_temp += [0]\n",
    "                    elif max(reference_count_temp, alter_count_temp)/float(total_count_temp) < 0.99:\n",
    "                        mapping_count_list_temp += [0]\n",
    "                    else:\n",
    "                        if max(reference_count_temp, alter_count_temp) == reference_count_temp:\n",
    "                            mapping_count_list_temp += [1]\n",
    "                        elif max(reference_count_temp, alter_count_temp) == alter_count_temp:\n",
    "                            mapping_count_list_temp += [-1]\n",
    "                        else:\n",
    "                            print \"something wrong\"\n",
    "                sequence_list += [mapping_count_list_temp]\n",
    "                \n",
    "    snp_pd = pd.DataFrame({'1': sequence_list[0]})\n",
    "    for i in range(len(sequence_list)):\n",
    "        snp_pd[str(i+1)] = sequence_list[i]\n",
    "    snp_pd = snp_pd.T\n",
    "    snp_pd.columns = barcode_list\n",
    "    snp_pd['contig_list'] = contig_list\n",
    "    snp_pd['location_on_contig_list'] = location_on_contig_list\n",
    "    snp_pd['reference_list'] = reference_list\n",
    "    snp_pd['alter_list'] = alter_list\n",
    "    snp_pd.to_csv(work_dirt_temp + file_name + '_snp.csv',index=False)\n",
    "    # this is the parsed SNP dataframe, \n",
    "    # 0 means no coverage, \n",
    "    # 1 means same as reference, \n",
    "    # -1 means same as alternate sequence\n",
    "    \n",
    "    # drop those alleles that have less than cutoff location counted as mutation\n",
    "    snp_pd = pd.read_csv(work_dirt_temp + file_name + '_snp.csv')\n",
    "    \n",
    "    to_drop_row_list = []\n",
    "    for i in range(len(snp_pd)):\n",
    "        if sum(snp_pd.iloc[i][:-4]!=0) <cutoff_cell_count:\n",
    "            to_drop_row_list.append(i)\n",
    "            continue\n",
    "        if sum(snp_pd.iloc[i][:-4]==-1) <cutoff_type_count or sum(snp_pd.iloc[i][:-4]==1) <cutoff_type_count :\n",
    "            to_drop_row_list.append(i)\n",
    "    print \"File name is {}, total SNPs before drop is {}, SNPs to drop is {}.\"\\\n",
    "    .format(file_name, len(snp_pd), len(to_drop_row_list))\n",
    "    \n",
    "    snp_pd = snp_pd.drop(to_drop_row_list, axis=0)\n",
    "    snp_pd = snp_pd.reset_index(drop=True)\n",
    "    snp_pd.to_csv(work_dirt_temp + file_name + '_snp_filtered.csv',index=False)\n",
    "    # this is the dataframe with less covered SNPs removed\n",
    "    \n",
    "    # This is to remove cells with less SNPs covered\n",
    "    snp_pd = pd.read_csv(work_dirt_temp + file_name + '_snp_filtered.csv')\n",
    "    cutoff_SNP_count = max(cutoff_SNP_ratio * len(snp_pd), 10)\n",
    "    to_drop_column_list = []\n",
    "    barcode_list = list(snp_pd.columns.values[:-4])\n",
    "    for i in range(snp_pd.shape[1]-4):\n",
    "        if sum(snp_pd[barcode_list[i]]!=0)< cutoff_SNP_count:\n",
    "             to_drop_column_list += [barcode_list[i]]\n",
    "                \n",
    "    print \"Total cells before drop is {}, cells to drop is {}.\"\\\n",
    "    .format(snp_pd.shape[1]-4, len(to_drop_column_list))\n",
    "    \n",
    "    snp_pd = snp_pd.drop(to_drop_column_list, axis=1)\n",
    "    snp_pd.to_csv(work_dirt_temp + file_name + '_snp_filtered_bc_dropped.csv',index=False)    \n",
    "    \n",
    "    \n",
    "    # this is to plot SNP ratio that fits to reference\n",
    "    snp_pd = pd.read_csv(work_dirt_temp + file_name + '_snp_filtered_bc_dropped.csv')\n",
    "    total_ratio_of_snps_list = []\n",
    "    for i in range(len(snp_pd)):\n",
    "        total_coverage_temp = sum(list(snp_pd.iloc[i][:-4]!=0))\n",
    "        if total_coverage_temp == 0:\n",
    "            print i\n",
    "            total_ratio_of_snps_list+= [1]\n",
    "            continue    \n",
    "        total_ratio_of_snps_list += [sum(list(snp_pd.iloc[i][:-4]==1))/float(total_coverage_temp)]\n",
    "    plt.figure()\n",
    "    count_temp = plt.hist(np.array(total_ratio_of_snps_list),bins = np.arange(0,1.01,0.02))\n",
    "    #plt.ylim(0,1.1*max(count_temp[0][:45]))\n",
    "    plt.tick_params(axis='both', direction=\"in\", which='both', top=True, right=True)\n",
    "    plt.xlabel(\"Ratio of cells matching strain 1\")\n",
    "    plt.ylabel(\"Number of alleles\")\n",
    "    plt.savefig(work_dirt_temp+\\\n",
    "                file_name+'_SNPs.svg', format='svg',bbox_inches='tight')\n",
    "    return(total_ratio_of_snps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dirt_temp = '/media/z/feb2021/microbe_seq_analysis/Q5_variant_calling_100fewer/raw_vcf_files/'\n",
    "# this is the newest dirtecorty\n",
    "file_list_temp = os.listdir(work_dirt_temp)\n",
    "list_of_ratio_lists = []\n",
    "for file_temp in file_list_temp:\n",
    "    if \".vcf\" == file_temp[-4:]:\n",
    "        total_ratio_of_snps_list = strain_split(file_temp,work_dirt_temp = work_dirt_temp)\n",
    "        list_of_ratio_lists.append(total_ratio_of_snps_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_strain(file_name, tree_cut_threshold = 2.7):\n",
    "    #file_name = vcf_files_list[file_index]#'59_calls_snps_qual30.vcf'\n",
    "    print file_name\n",
    "    work_dirt_temp = '/media/z/feb2021/microbe_seq_analysis/Q5_variant_calling_100fewer/raw_vcf_files/'\n",
    "    snp_pd = pd.read_csv(work_dirt_temp + file_name + '_snp_filtered_bc_dropped.csv')\n",
    "    print(len(snp_pd))\n",
    "    snp_pd_temp = snp_pd\n",
    "    snp_pd_temp = snp_pd_temp.drop(['contig_list','location_on_contig_list', 'reference_list', 'alter_list'], axis=1)\n",
    "    snp_pd_temp = snp_pd_temp.T\n",
    "    #X = snp_pd_temp\n",
    "    X=preprocessing.normalize(snp_pd_temp)\n",
    "    linked = linkage(X, method='ward')\n",
    "    matplotlib.rcParams.update({'font.size': 50})\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(3,9))\n",
    "\n",
    "    #ax.set_title('Hierarchical Clustering of Cells')\n",
    "    #ax.set_ylabel('Distance')\n",
    "    #ax.set_xlabel(\"Single cells\")\n",
    "    plt.yticks([0,50,100,150,200])\n",
    "    plt.tick_params(axis='both', direction=\"in\", which='both',right=True,length =15,width=3)\n",
    "    matplotlib.rcParams['lines.linewidth'] = 0.8\n",
    "\n",
    "    #hierarchy.set_link_color_palette(['#F16A70', '#4D4D4D', '#B1D877', '#8CDCDA',])\n",
    "    strain_color_list_BV = [strain_color_list[1], strain_color_list[0], strain_color_list[2],strain_color_list[1]]\n",
    "    hierarchy.set_link_color_palette(strain_color_list_BV)\n",
    "    dendrogram_plot = dendrogram(linked,  \n",
    "                                 orientation='left',\n",
    "                                 distance_sort='ascending',\n",
    "                                 leaf_font_size = 2,\n",
    "                                 no_labels=True,\n",
    "                                 color_threshold = tree_cut_threshold,\n",
    "                                 above_threshold_color = 'black',\n",
    "                                 show_leaf_counts=False)\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.gca().spines.values()[0].set_visible(False)\n",
    "    plt.gca().spines.values()[1].set_visible(False)\n",
    "    plt.gca().spines.values()[2].set_visible(False)\n",
    "    plt.gca().spines.values()[3].set_visible(False)\n",
    "    plt.savefig(file_name.split(\"_\")[0]+'_SNP_similarity_tree.svg', format='svg',bbox_inches='tight')\n",
    "    matplotlib.rcParams.update({'font.size': 12})\n",
    "    \n",
    "    plt.figure()\n",
    "    X = snp_pd_temp\n",
    "    X=preprocessing.normalize(snp_pd_temp)\n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    embedding = reducer.fit_transform(X)\n",
    "    embedding.shape\n",
    "    plt.scatter(embedding[:,0], embedding[:,1],s=3)\n",
    "    plt.savefig(file_name.split(\"_\")[0]+'_SNP_umap.svg', format='svg',bbox_inches='tight')\n",
    "    # this is to construct a SNP similarity matrix from data\n",
    "    # the sequence is based on the sequence of dendragram\n",
    "    index_int_list = []\n",
    "    for i in dendrogram_plot['ivl']:\n",
    "        index_int_list.append(int(i))\n",
    "    SNP_similarity_df = pd.DataFrame\\\n",
    "    (-1.0, index=snp_pd.columns[index_int_list], columns=snp_pd.columns[index_int_list])\n",
    "    for i in range(len(SNP_similarity_df)):\n",
    "        column_name_a = snp_pd.columns[index_int_list[i]]\n",
    "        for j in range(len(SNP_similarity_df)):\n",
    "            if i == j:\n",
    "                SNP_similarity_df.iloc[i][j] = 1.0\n",
    "                continue\n",
    "            column_name_b = snp_pd.columns[index_int_list[j]]\n",
    "            check_snp_temp = snp_pd[[column_name_a, column_name_b]]\n",
    "            check_snp_temp = check_snp_temp.loc[check_snp_temp[column_name_b]!=0]\n",
    "            check_snp_temp = check_snp_temp.loc[check_snp_temp[column_name_a]!=0]\n",
    "            if len(check_snp_temp) == 0:# if there is no overlap between 2 cells, give it -1 \n",
    "                continue\n",
    "            SNP_similarity_df.iloc[i][j] = sum(check_snp_temp[column_name_b] == check_snp_temp[column_name_a])/\\\n",
    "            float(len(check_snp_temp))\n",
    "    SNP_similarity_df.to_csv(work_dirt_temp+file_name.split(\"_\")[0]+'_SNP_similarity_df.csv', index = False)    \n",
    "    fig, ax = plt.subplots()\n",
    "    data_color = []\n",
    "    fig.set_size_inches(6,6, forward=True)\n",
    "    my_cmap = plt.cm.get_cmap(matplotlib.cm.cmap_d.keys()[58])\n",
    "    my_cmap = mpl.colors.ListedColormap(['w', default_color_list[4]])\n",
    "    my_cmap = 'RdBu'\n",
    "    maximum_similarity = 1\n",
    "    SNP_similarity_df = pd.read_csv(work_dirt_temp+file_name.split(\"_\")[0]+'_SNP_similarity_df.csv')\n",
    "    ax.imshow(SNP_similarity_df, cmap=my_cmap, vmax=maximum_similarity, vmin=0)\n",
    "\n",
    "    ax.set_xlabel(\"Single cells\", fontsize=15)\n",
    "    #ax.set_ylabel(\"Assembled genomes 1 to 52\", fontsize=15)\n",
    "    sm = ScalarMappable(cmap=my_cmap, norm=plt.Normalize(0,maximum_similarity))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, orientation = 'vertical',fraction = 0.05, pad = 0.05, \\\n",
    "                        aspect = 15, ticks = [0, maximum_similarity])\n",
    "    cbar.set_label('Similarity of SNPs between cells', rotation=90,labelpad=-1,fontsize=15)\n",
    "    plt.savefig(file_name.split(\"_\")[0]+'_SNP_similarity.svg', format='svg',bbox_inches='tight')\n",
    "    return(snp_pd, linked, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_check = \"17_calls_snps_qual30.vcf\"\n",
    "snp_pd, linked, embedding=check_strain(file_name=file_name_check, tree_cut_threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_number = 3\n",
    "cutree_contigs = cut_tree(linked, n_clusters=cluster_number)    \n",
    "print len(cutree_contigs), sum(cutree_contigs==[0]),sum(cutree_contigs==[1]),sum(cutree_contigs==[2])\n",
    "\n",
    "umap_compare = pd.DataFrame({'Barcode':snp_pd.columns[:-4]})\n",
    "umap_compare['X'] = embedding[:,0]\n",
    "umap_compare['Y'] = embedding[:,1]\n",
    "cluster_hier = []\n",
    "for i in range(len(cutree_contigs)):   \n",
    "    cluster_hier.append(cutree_contigs[i][0])\n",
    "cluster_umap = []\n",
    "for i in range(len(umap_compare)):\n",
    "    if umap_compare['X'][i]>0 and umap_compare['Y'][i]>1:\n",
    "        cluster_umap.append(1)\n",
    "    elif umap_compare['X'][i]<-5 and umap_compare['Y'][i]<-0.7:\n",
    "        cluster_umap.append(2)\n",
    "    elif umap_compare['X'][i]<-5 and umap_compare['Y'][i]>-0.7:\n",
    "        cluster_umap.append(0)\n",
    "   # else:\n",
    "    #    cluster_umap.append('B')\n",
    "umap_compare['cluster_umap'] = cluster_umap\n",
    "umap_compare['cluster_hier'] = cluster_hier     \n",
    "#print sum(umap_compare['cluster_umap'] == umap_compare['cluster_hier'])/float(len(umap_compare['cluster_umap']))\n",
    "umap_compare.loc[umap_compare['cluster_umap'] != umap_compare['cluster_hier']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(cluster_number):    \n",
    "    umap_compare_bin = umap_compare.loc[umap_compare['cluster_hier']==j]\n",
    "    plt.scatter(umap_compare_bin['X'], umap_compare_bin['Y'],s=10,c=strain_color_list[j],marker=umap_plot_shape[j])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.ylabel('UMAP dimension 2')\n",
    "plt.xlabel(\"UMAP dimension 1\")\n",
    "plt.savefig(file_name_check.split(\"_\")[0]+'_SNP_umap_final.svg', format='svg',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_number = 3\n",
    "cutree_contigs = cut_tree(linked, n_clusters=cluster_number)    \n",
    "len(cutree_contigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to construct the genotype list of each strain\n",
    "\n",
    "for i in range(cluster_number):\n",
    "    index_of_this_cluster = [] \n",
    "    for j in range(len(cutree_contigs)):\n",
    "        if cutree_contigs[j] == i:\n",
    "            index_of_this_cluster += [j] \n",
    "    snp_pd['hierarchy_cluster_'+str(i)+'pos']=(snp_pd[snp_pd.columns.values[index_of_this_cluster]]==1).sum(axis=1)\n",
    "    snp_pd['hierarchy_cluster_'+str(i)+'neg']=(snp_pd[snp_pd.columns.values[index_of_this_cluster]]==-1).sum(axis=1)\n",
    "    \n",
    "strain_X_genotype = [[] for i in range(cluster_number)]\n",
    "for i in range(len(snp_pd)):\n",
    "    for j in range(cluster_number):\n",
    "        # generate genotype of each strain\n",
    "        # at each SNP, if both have less than 2 cells, assign it 0, means not sure\n",
    "        # if the max genotype is less than 90% total, assign 0\n",
    "        # otherwise assign the major genotype\n",
    "        if snp_pd['hierarchy_cluster_'+str(j)+'pos'][i] <= 1 and snp_pd['hierarchy_cluster_'+str(j)+'neg'][i] <=1:\n",
    "            strain_X_genotype[j].append(0)\n",
    "            continue\n",
    "        elif snp_pd['hierarchy_cluster_'+str(j)+'pos'][i]!= 0 and snp_pd['hierarchy_cluster_'+str(j)+'neg'][i] != 0:\n",
    "            if max(snp_pd['hierarchy_cluster_'+str(j)+'pos'][i],snp_pd['hierarchy_cluster_'+str(j)+'neg'][i])/\\\n",
    "            float(min(snp_pd['hierarchy_cluster_'+str(j)+'pos'][i],snp_pd['hierarchy_cluster_'+str(j)+'neg'][i]))\\\n",
    "            < 9:\n",
    "                strain_X_genotype[j].append(0)\n",
    "                continue                \n",
    "        if snp_pd['hierarchy_cluster_'+str(j)+'pos'][i] > snp_pd['hierarchy_cluster_'+str(j)+'neg'][i]:\n",
    "            strain_X_genotype[j].append(1)\n",
    "        else:\n",
    "            strain_X_genotype[j].append(-1)\n",
    "    \n",
    "for j in range(cluster_number):\n",
    "    snp_pd['strain_' + str(j) + '_genotype'] = strain_X_genotype[j]\n",
    "\n",
    "# drop SNPs without information (a SNP with both 1 and -1 is informative)\n",
    "rows_to_drop = []\n",
    "for i in range(len(snp_pd)):\n",
    "    genotype_SNP_temp = []\n",
    "    for j in range(cluster_number):\n",
    "        genotype_SNP_temp.append(snp_pd['strain_' + str(j) + '_genotype'][i])\n",
    "    if not (1 in genotype_SNP_temp and -1 in genotype_SNP_temp):\n",
    "        rows_to_drop.append(i)\n",
    "print \"SNPs before drop is {}, SNPs to drop is {}.\".format(len(snp_pd), len(rows_to_drop))\n",
    "snp_pd = snp_pd.drop(rows_to_drop, axis=0)\n",
    "snp_pd = snp_pd.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to generate strain list\n",
    "list_of_ratio = [[] for i in range(cluster_number)]\n",
    "barcode_list_strain = [[] for i in range(cluster_number+1)] # the last strain is those not sure\n",
    "for i in range(snp_pd.shape[1]):\n",
    "    column_name_temp = snp_pd.columns[i]\n",
    "    if not column_name_temp.isdigit(): # this means that this is not a barcode name\n",
    "        continue\n",
    "    if not column_name_temp in snp_pd.columns:\n",
    "        continue\n",
    "    temp_df_check = snp_pd.loc[snp_pd[column_name_temp]!=0]\n",
    "    for j in range(cluster_number):\n",
    "        temp_df_check_strain = temp_df_check.loc[temp_df_check['strain_' + str(j) + '_genotype']!=0]\n",
    "        total_count_same = sum(temp_df_check_strain[column_name_temp] == \\\n",
    "        temp_df_check_strain['strain_' + str(j) + '_genotype'])\n",
    "        list_of_ratio[j].append(total_count_same/float(len(temp_df_check_strain[column_name_temp])))\n",
    "    list_of_ratio_check = [list_of_ratio[j][-1] for j in range(cluster_number)]\n",
    "    if max(list_of_ratio_check) < 0.95:\n",
    "        barcode_list_strain[-1].append(column_name_temp)\n",
    "    elif sum(np.array(list_of_ratio_check)>0.95) > 1:\n",
    "        print \"Not sure, more than two strains match\", column_name_temp\n",
    "        barcode_list_strain[-1].append(column_name_temp)\n",
    "    else:\n",
    "        barcode_list_strain[list_of_ratio_check.index(max(list_of_ratio_check))].append(column_name_temp)\n",
    "for i in range(len(barcode_list_strain)):\n",
    "    print i, len(barcode_list_strain[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to count cell number in these 7 time points\n",
    "time_points = [14,52,111,150,171,172,224] # this is in sample ID\n",
    "# in days\n",
    "time_points = [15,104,276,367,404,405,526] # this is in days\n",
    "time_point_index_convcert = {14:0,\n",
    "                             31:1,\n",
    "                             32:1,\n",
    "                             34:1,\n",
    "                             8:2,\n",
    "                             9:2,\n",
    "                             5:3,\n",
    "                             7:3,\n",
    "                             15:3,\n",
    "                             27:3,\n",
    "                             10:4,\n",
    "                             11:5,\n",
    "                             12:5,\n",
    "                             33:6,\n",
    "                             35:6\n",
    "                            }\n",
    "number_of_strains = [[0]*7 for i in range(cluster_number)]\n",
    "for i in range(cluster_number):\n",
    "    for j in range(len(barcode_list_strain[i])): \n",
    "        barcode_temp = int(barcode_list_strain[i][j])\n",
    "        time_point_index_temp = time_point_index_convcert[barcode_temp/100000]\n",
    "        number_of_strains[i][time_point_index_temp] += 1\n",
    "total_cells_known = [0] * 7\n",
    "for i in range(len(time_points)):\n",
    "    for j in range(cluster_number):\n",
    "        total_cells_known[i] += float(number_of_strains[j][i])\n",
    "        \n",
    "color_1 = [177.0/255, 216.0/255, 119.0/255]\n",
    "color_2 = [77.0/255, 77.0/255, 77.0/255]\n",
    "color_3 = [241.0/255, 106.0/255, 112.0/255]\n",
    "color_4 = [140.0/255, 220.0/255, 218.0/255]\n",
    "color_list = [color_1]+[color_2]+[color_3]+[color_4]\n",
    "marker_list = ['>', 'D', 's', 'o']\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 15})\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(cluster_number):\n",
    "    ax.plot(time_points,  np.array(number_of_strains[i])/np.array(total_cells_known),\\\n",
    "            color=color_list[i],linewidth=2)\n",
    "    ax.scatter(time_points, np.array(number_of_strains[i])/np.array(total_cells_known),\\\n",
    "               color=color_list[i],marker=marker_list[i], s = 35)\n",
    "plt.xlabel(\"Time (days)\")\n",
    "plt.ylabel(\"Fraction of strains\")\n",
    "plt.ylim(-0.1,1.1)\n",
    "\n",
    "plt.xticks(time_points)\n",
    "ax.set_xticklabels(['15','104','276','367','','     405','526'])\n",
    "\n",
    "strain_1_legend = mlines.Line2D([], [], color=color_list[0], marker=marker_list[0], linestyle='-',\n",
    "                          markersize=7, label=\"strain 1\", linewidth=2)\n",
    "strain_2_legend = mlines.Line2D([], [], color=color_list[1], marker=marker_list[1], linestyle='-',\n",
    "                          markersize=7, label=\"strain 2\", linewidth=2)\n",
    "strain_3_legend = mlines.Line2D([], [], color=color_list[2], marker=marker_list[2], linestyle='-',\n",
    "                          markersize=7, label=\"strain 3\", linewidth=2)\n",
    "strain_4_legend = mlines.Line2D([], [], color=color_list[3], marker=marker_list[3], linestyle='-',\n",
    "                          markersize=7, label=\"strain 4\", linewidth=2)\n",
    "#plt.legend(handles=[strain_0_legend, strain_1_legend, strain_2_legend, strain_3_legend], \\\n",
    "#           loc='upper left', fontsize=12, framealpha=0.5)\n",
    "plt.tick_params(axis='both', direction=\"in\", which='both', top=True, right=True)\n",
    "legend1 = plt.legend(handles=[strain_1_legend,strain_2_legend,strain_3_legend], \\\n",
    "           loc='best', fontsize=12, framealpha=0.5)\n",
    "#plt.legend(handles=[strain_3_legend, strain_4_legend], \\\n",
    "#           loc='upper center', fontsize=12, framealpha=0.5)\n",
    "plt.gca().add_artist(legend1)\n",
    "plt.savefig(file_name_check.split(\"_\")[0]+'_time_plot.svg', format='svg',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirt_fastq = \"/media/z/feb2021/04082019Nextseq/fastq_SE/\"\n",
    "strain_assembly_dirt = \"/media/z/feb2021/microbe_seq_analysis/Q5_variant_calling_100fewer/strain_assembly/\"\n",
    "for strain_to_check in range(cluster_number):\n",
    "    assembly_dirt_temp = strain_assembly_dirt + file_name_check.split('_')[0]+'_'+str(strain_to_check)+'/'\n",
    "    os.mkdir(assembly_dirt_temp)\n",
    "    for bc_temp in barcode_list_strain[strain_to_check]:\n",
    "        !cp {dirt_fastq}{bc_temp}.fastq {assembly_dirt_temp}\n",
    "    !cat {assembly_dirt_temp}*.fastq > {assembly_dirt_temp}all.fq\n",
    "    !rm {assembly_dirt_temp}*.fastq\n",
    "    !(spades.py --sc --careful --pe1-s {assembly_dirt_temp}all.fq -o {assembly_dirt_temp}all_scCareful) \\\n",
    "    1> {strain_assembly_dirt}/{file_name_check.split('_')[0]}_{str(strain_to_check)}.txt\n",
    "    !cp {assembly_dirt_temp}all_scCareful/contigs.fasta \\\n",
    "    {strain_assembly_dirt}/{file_name_check.split('_')[0]}_{str(strain_to_check)}.fasta\n",
    "    !rm -r {assembly_dirt_temp}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
